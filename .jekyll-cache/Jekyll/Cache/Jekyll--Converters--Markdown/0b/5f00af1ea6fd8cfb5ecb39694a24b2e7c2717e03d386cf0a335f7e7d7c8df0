I"	@<font color="red">此篇仅使用Tensorflow 低级的API,不适用高级的API(tf.keras)</font>

<h2 id="tensorflow-20同1x之间的重要区别">TensorFlow 2.0同1.x之间的重要区别</h2>

<ul>
  <li>在API层面的类、方法有了较大的变化，这个需要在使用中慢慢熟悉</li>
  <li>取消了Session机制，每一条命令直接执行，而不需要等到Session.run</li>
  <li>因为取消了Session机制，原有的数学模型定义，改为使用Python函数编写。原来的feed_dict和tf.placeholder，成为了函数的输入部分；原来的fetches，则成为了函数的返回值</li>
  <li>使用keras的模型体系对原有的TensorFlow API进行高度的抽象，使用更容易</li>
  <li>使用tf.keras.Model.fit来替代原有的训练循环</li>
</ul>

<h2 id="1-安装了v2x的tensorflow使用v1x的代码">1. 安装了V2.x的tensorflow,使用V1.x的代码</h2>
<h3 id="11-使用tensorflowcompatv1全局替换">1.1 使用tensorflow.compat.v1全局替换</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_v2_behavior</span><span class="p">()</span>

</code></pre></div></div>
<h3 id="12-使用tfcompatv1xxxx替代tfxxxx">1.2 使用tf.compat.v1.xxxx替代tf.xxxx</h3>
<p>修复下列报错</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AttributeError: module <span class="s1">'tensorflow'</span> has no attribute xxxx
</code></pre></div></div>

<p>注: 从TensorFlow2.0开始，默认情况下启用了Eager Execution;Tensorflow1.x 许添加相关代码执行.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 启用动态图机制
</span><span class="n">tf</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>
</code></pre></div></div>
<h2 id="2-安装了v2x的tensorflow移植v1x的代码至v2x">2. 安装了V2.x的tensorflow,移植V1.x的代码至V2.x</h2>
<h3 id="21-使用迁移工具tf_upgrade_v2转换v1x代码至v2x代码">2.1 使用迁移工具tf_upgrade_v2转换V1.x代码至V2.x代码</h3>
<ul>
  <li><strong>命令</strong>
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 转化整个工程</span>
tf_upgrade_v2 <span class="nt">--intree</span> my_project/ <span class="nt">--outtree</span> my_project_v2/ <span class="nt">--reportfile</span> report.txt
<span class="c"># 转化单个文件</span>
tf_upgrade_v2 <span class="nt">--infile</span> first-tf.py <span class="nt">--outfile</span> first-tf-v2.py
</code></pre></div>    </div>
  </li>
  <li><strong>效果</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'loss'</span><span class="p">):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ys</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">),</span><span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div>    </div>
    <p>变成如下:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'loss'</span><span class="p">):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ys</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div>    </div>
  </li>
  <li><strong>结论</strong>
    <ul>
      <li>使用tf.compat.v1.xxxx替代tf.xxxx</li>
      <li>函数入参变化</li>
    </ul>
  </li>
</ul>

<h3 id="22-重构v1x代码">2.2 重构V1.x代码</h3>

<p>V1.x代码</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="n">y_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_value</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
<span class="c1"># TensorFlow内置的梯度下降算法，每步长0.5
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># 代价函数值最小化的时候，代表求得解
</span><span class="n">train</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1"># 初始化所有变量，也就是上面定义的a/b两个变量
</span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c1"># 启动图
</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="c1"># 真正的执行初始化变量，还是老话，上面只是定义模型，并没有真正开始执行
</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="c1"># 重复梯度下降200次，每隔5次打印一次结果
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span>
        <span class="n">step</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<p>重构后</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="o">@</span><span class="n">tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
<span class="c1"># 定义代价函数，也是python函数
</span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">predicted_y</span><span class="p">,</span> <span class="n">desired_y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">predicted_y</span> <span class="o">-</span> <span class="n">desired_y</span><span class="p">))</span>

<span class="c1"># TensorFlow内置Adam算法，每步长0.1
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># 还可以选用TensorFlow内置SGD(随机最速下降)算法，每步长0.001
# 不同算法要使用适当的步长，步长过大会导致模型无法收敛
# optimizer = tf.optimizers.SGD(0.001)
</span>
<span class="c1"># 重复梯度下降200次，每隔5次打印一次结果
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 进行一次计算
</span>        <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># 得到当前损失值
</span>        <span class="n">grads</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">current_loss</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>  <span class="c1"># 调整模型中的权重、偏移值
</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>  <span class="c1"># 调整之后的值代回到模型
</span>    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># 每5次迭代显示一次结果
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"Step:</span><span class="si">%</span><span class="s">d loss:</span><span class="si">%%%2.5</span><span class="s">f weight:</span><span class="si">%2.7</span><span class="s">f bias:</span><span class="si">%2.7</span><span class="s">f "</span> <span class="o">%</span>
              <span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">current_loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>

</code></pre></div></div>
:ET